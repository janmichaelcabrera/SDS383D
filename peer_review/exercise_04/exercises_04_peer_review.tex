\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[letterpaper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{courier}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.96,0.96,0.96}
\lstset{
    backgroundcolor=\color{lbcolor},
    tabsize=4,
    rulecolor=,
    language=Python,
        basicstyle=\footnotesize\ttfamily,
        upquote=true,
        aboveskip={1.0\baselineskip},
        columns=fixed,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\newcommand{\by}{\mathbf{y}}


\begin{document}

    \title{SDS 383D, Exercises 4: Peer Review for Yilin He}
    \author{Jan-Michael Cabrera}
    \date{\today}
    \maketitle

    \section*{Price elasticity of demand}

    \subsection*{Part I: data preparation}

    I like the idea of using training data and testing data for evaluating the fit of a model. Although each store does have more than 50 samples, for some stores the effect of having a display or not may not be captured correctly in your testing data since some of the stores have no data for this effect. 

    I like your idea here of "rectangulating" your data set. It's not something I've thought of before.

    \subsection*{Part II: fit the Bayesian linear model}

    I also initially looked at getting the full conditional for each $\beta_{0i}$, $\beta_{1i}$, etc. Deriving that result was a pain, wasn't it? I highly recommend collecting each $\beta$ coefficient into a vector:

    $$\beta_i = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots  \end{bmatrix}_i$$

    \noindent This makes deriving the full conditional much, much easier and also makes the code run much faster. Of course you'll also need to collect your $x_{ij}$'s into a matrix of $X_i$'s. 

    

    \subsection*{Part III Comparison: bayesian v.s. 16 random effects models}

    \section*{General Comments}

    \begin{itemize}

      \item Overall, the derivations are very good and concise. Using larger parenthesis/brackets in places will definitely make it a bit easier on the eyes.

      \item In my opinion, I think a bit more exposition in the derivations would help make the derivations more readable. What you did in 'Bayesian CI under Non-informative Prior' was really good. More of this throughout would be awesome.

      \item The code is relatively easy to follow, but could be packaged to be more useful to you in the future. 

      \item This was very good. Keep it up!

    \end{itemize}

\end{document}