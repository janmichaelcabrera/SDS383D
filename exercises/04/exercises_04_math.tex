\documentclass[10pt]{article}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[letterpaper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{courier}
\usepackage{subcaption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.96,0.96,0.96}
\lstset{
    backgroundcolor=\color{lbcolor},
    tabsize=4,
    rulecolor=,
    language=Python,
        basicstyle=\footnotesize\ttfamily,
        upquote=true,
        aboveskip={1.0\baselineskip},
        columns=fixed,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
}


\begin{document}

    \title{SDS 383D, Exercises 4: Hierarchical Models: Data-analysis Problems}
    \author{Jan-Michael Cabrera}
    \date{\today}
    \maketitle

    \section*{Math tests}

    The data set in ``mathtest.csv'' shows the scores on a standardized math test from a sample of 10th-grade students at 100 different U.S.~urban high schools, all having enrollment of at least 400 10th-grade students.  (A lot of educational research involves ``survey tests'' of this sort, with tests administered to all students being the rare exception.)

    Let $\theta_i$ be the underlying mean test score for school $i$, and let $y_{ij}$ be the score for the $j$th student in school $i$.  Starting with the ``mathtest.R'' script, you'll notice that the extreme school-level averages $\bar{y}_i$ (both high and low) tend to be at schools where fewer students were sampled.

    \begin{enumerate}
        \item Explain briefly why this would be.

        Using the arithmetic mean as an approximation for the true mean is not very good for small sample sizes. The influence of relatively extreme values in the sample can greatly throw off the estimation of the mean for small sample sizes.

        \item Fit this normal hierarchical model to these data via Gibbs sampling:
        \begin{eqnarray*}
        y_{ij} &\sim& \mbox{N}(\theta_i, \sigma^2) \\
        \theta_i &\sim& \mbox{N}(\mu, \tau^2 \sigma^2)
        \end{eqnarray*}
        Decide upon sensible priors for the unknown model parameters $(\mu, \sigma^2, \tau^2)$.

        \begin{align*}
            \pi_j(\mu) &= 1 \\
            \pi_j(\sigma^2) &= 1/\sigma^2 \\
            \pi_j(\tau^2) &= 1/\tau^2
        \end{align*}

        $$p(y_{ij} | \theta_i, \sigma^2) \propto \left(\frac{1}{\sigma^2}\right)^{1/2} \text{exp}\left[-\frac{1}{2 \sigma^2} (\theta_i - y_{ij})^2 \right]$$

        \begin{align*}
            p(\mathbf{y} | \theta_1, \dots, \theta_m, \sigma^2) & \propto \prod_{i=1}^m \prod_{j=1}^{n_i} \left(\frac{1}{\sigma^2}\right)^{1/2} \text{exp}\left[-\frac{1}{2 \sigma^2} (\theta_i - y_{ij})^2 \right] \\
            & \propto \left(\frac{1}{\sigma^2}\right)^{N/2} \text{exp}\left[-\frac{1}{2\sigma^2} \sum_{i=1}^m \sum_{j=1}^{n_i} (\theta_i = y_{ij})^2 \right]
        \end{align*}

        $$p(\theta_i | \mu, \tau^2, \sigma^2) \propto \left(\frac{1}{\tau^2 \sigma^2} \right)^{1/2} \text{exp}\left[-\frac{1}{2 \tau^2 \sigma^2} (\theta_i - \mu)^2 \right]$$

        $$p(\theta | \mu, \tau^2, \sigma^2) \propto \left(\frac{1}{\tau^2 \sigma^2} \right)^{m/2} \text{exp}\left[-\frac{1}{\tau^2 \sigma^2} \sum_{i=1}^m (\theta_i - \mu)^2 \right]$$

        \begin{align*}
            p(\tau^2 | \theta, \mu, \sigma^2) &\propto \pi_j(\tau^2) p(\theta | \sigma^2 \tau^2 \mu) \\
            &\propto \left(\frac{1}{\tau^2}\right)^{1/2} \left(\frac{1}{\tau^2 \sigma^2} \right)^{m/2} \text{exp}\left[-\frac{1}{2\tau^2 \sigma^2} \sum_{i=1}^m (\theta_i - \mu)^2 \right] \\
            &\propto \left(\frac{1}{\tau^2}\right)^{m/2+1} \text{exp}\left[-\frac{1}{\tau^2} \left(\frac{1}{2\sigma^2 } \sum_{i=1}^m (\theta_i - \mu)^2 \right) \right]
        \end{align*}

        $$\tau^2 | \theta, \mu, \sigma^2 \sim IG\left(\frac{m}{2}, \frac{1}{2\sigma^2 } \sum_{i=1}^m (\theta_i - \mu)^2 \right)$$

        \begin{align*}
            p(\sigma^2 | \theta, \mu, \tau^2, y) &\propto \pi_j(\sigma^2) p(\theta|\sigma^2 \tau^2 \mu) p(\mathbf{y}| \theta, \sigma^2) \\
            &\propto \frac{1}{\sigma^2} \left(\frac{1}{\tau^2 \sigma^2} \right)^{m/2} \text{exp}\left[-\frac{1}{2\tau^2 \sigma^2} \sum_{i=1}^m (\theta_i - \mu)^2 \right] \left(\frac{1}{\sigma^2}\right)^{N/2} \text{exp}\left[-\frac{1}{2\sigma^2} \sum_{i=1}^m \sum_{j=1}^{n_i} (\theta_i = y_{ij})^2 \right] \\
            &\propto \left(\frac{1}{\sigma^2} \right)^{\frac{m+N}{2} + 1} \text{exp}\left[-\frac{1}{\sigma^2} \left( \frac{1}{2\tau^2} \sum_{i+1}^m + \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^{n_i} (\theta_i - y_{ij})^2 \right) \right]
        \end{align*}

        $$\sigma^2 | \theta, \mu, \tau^2, y \sim IG\left(\frac{m+N}{2} + 1, \frac{1}{2\tau^2} \sum_{i+1}^m + \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^{n_i} (\theta_i - y_{ij})^2 \right)$$

        \begin{align*}
            p(\theta_i | \mu, \tau^2, \sigma^2, \mathbf{y}) &\propto p(\theta_i | \sigma^2, \tau^2, \mu) p(\bar{y}_i | \theta_i \sigma^2) \\
            &\propto \left(\frac{1}{\tau^2 \sigma^2} \right)^{1/2} \text{exp}\left[-\frac{1}{2 \tau^2 \sigma^2} (\theta_i - \mu)^2 \right] \left( \frac{1}{\sigma^2}\right)^{n_i/2} \text{exp}\left[-\frac{n_i}{2 \sigma^2} (\theta_i - \bar{y}_i)^2 \right]
        \end{align*}

        $$\theta_i | \mu, \tau^2, \sigma^2, \mathbf{y} \sim N \left( \frac{\frac{n_i}{\sigma^2} \bar{y}_i + \frac{1}{\tau^2 \sigma^2} \mu}{\frac{n_i}{\sigma^2} + \frac{1}{\tau^2 \sigma^2}}, \left[ \frac{n_i}{\sigma^2} + \frac{1}{\tau^2 \sigma^2}\right]^{-1} \right)$$

        \item Suppose you use the posterior mean $\hat{\theta}_i$ from the above model to estimate each school-level mean $\theta_i$.  Define the shrinkage coefficient $\kappa_i$ as
        $$
        \kappa_i = \frac{ \bar{y}_i - \hat{\theta}_i}{\bar{y}_i} \, ,
        $$
        which tells you how much the posterior mean shrinks the observed sample mean.  Plot this shrinkage coefficient (in absolute value) for each school as a function of that school's sample size, and comment.

    \end{enumerate}


\end{document}