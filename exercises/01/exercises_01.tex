\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[letterpaper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{graphicx}%
\usepackage{hyperref}


\begin{document}

    \title{SDS 383D, Chapter 1: Preliminaries}
    \author{Jan-Michael Cabrera, JC7858}
    \date{\today}
    \maketitle

    \section*{Bayesian inference in simple conjugate families}

    \subsection*{A: Beta prior with Bernoulli sampling distribution}

        \begin{equation}
              p(w) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}w^{a-1}(1-w)^{b-1}
        \end{equation}

        \begin{equation}
              p(D|w) = \binom{n}{y} w^y (1 - w)^{n-y}
        \end{equation}

        \begin{equation}
              p(w|D) = p(w) \frac{p(D|w)}{p(D)} \propto p(w) p(D|w)
        \end{equation}

        \begin{align}
              p(w|D) &\propto w^{a-1}(1-w)^{b-1}w^y(1-w)^{n-y} \\
              p(w|D) &\propto w^{a+y-1}(1-w)^{n+b-y-1} \\
              w|d &\sim \text{Beta}(a+y, n+b-y)
        \end{align}


    \subsection*{B: Change of variables for a gamma random variable}

        \begin{equation}
              p(x) = \frac{b^a}{\Gamma(a)}x^{a-1} \text{exp}(-bx)
        \end{equation}

        \begin{align}
              x_1 &\sim \text{Ga}(a_1, 1)\\
              x_2 &\sim \text{Ga}(a_2, 1)
        \end{align}

        \begin{equation}
              f_{\mathbf{X}}(x_1, x_2) = \frac{x^{a_1-1}}{\Gamma(a_1)}\frac{x^{a_2-1}}{\Gamma(a_2)}x_1^{a_1-1} x_2^{a_2-1} e^{-x_1}e^{-x_2}
        \end{equation}

        \begin{equation}
              f_{\mathbf{Y}}(\mathbf{y}) = f_{\mathbf{X}}[\mathbf{h(y)}]\cdot|\text{det}(\nabla \mathbf{h(y}))|
        \end{equation}

        \begin{align}
              y_1 &= \frac{x_1}{(x_1+x_2)} \\
              y_2 &= x_1 + x_2
        \end{align}

        \begin{align}
              x_1 &= y_1 y_2 \\
              x_2 &= y_2 - y_1 y_2
        \end{align}

        \begin{equation}
              \text{det}(\nabla \mathbf{h(y)}) = \text{det}
              \begin{vmatrix}
              y_2 & y_1 \\
              -y_2 & 1-y_1
              \end{vmatrix}
              = y_2
        \end{equation}

        \begin{equation}
              f_{\mathbf{Y}}(y_1, y_2) = \frac{(y_1 y_2)^{a_1-1}}{\Gamma(a_1)}\frac{(y_2 (1-y_1))^{a_2-1}}{\Gamma(a_2)}(y_1 y_2)^{a_1-1} (y_2 (1-y_1))^{a_2-1} e^{-y_1 y_2}e^{-(y_2 - y_1 y_2)} y_2
        \end{equation}

        \begin{equation}
              f_{\mathbf{Y}}(y_1, y_2) = \frac{\Gamma(a_1 + a_2)}{\Gamma(a_1) \Gamma(a_2)} y_1^{a_1 -1} (1-y_1)^{a_2 - 1} \cdot \frac{1}{\Gamma(a_1 + a_2)} y_2^{a_1 + a_2 -1} e^{-y_2}
        \end{equation}

        \begin{align}
              y_1 &\sim \text{Beta}(a_1, a_2)\\
              y_2 &\sim \text{Ga}(a_1 + a_2, 1)
        \end{align}

    \subsection*{C: Normal prior and sampling distribution with unknown mean and known variance}

        \begin{equation}
              p(\theta|x_1,..., x_n) \propto p(\theta|m,v)p(x_1,...,x_n|\theta)
        \end{equation}

        \begin{equation}
              p(\theta|m,v) = \left(\frac{1}{2 \pi v} \right )^{1/2} \text{exp}\left( - \frac{1}{2v} (\theta - m)^2\right )
        \end{equation}

        \begin{equation}
              p(x_i|\theta) = \left(\frac{1}{2 \pi \sigma^2} \right )^{1/2} \text{exp}\left( - \frac{1}{2 \sigma^2} (x_i - \theta)^2\right )
        \end{equation}

        \begin{equation}
              p(x_1,...,x_n|\theta) = \prod_i^n p(x_i|\theta) = \left(\frac{1}{2 \pi \sigma^2} \right )^{n/2} \text{exp}\left( - \frac{1}{2 \sigma^2} \sum_i^n(x_i - \theta)^2\right )
        \end{equation}

        \begin{equation}
              p(\theta|x_1,...,x_n) \propto \text{exp}\left( - \frac{1}{2 \sigma^2} (x_i - \theta)^2\right ) \text{exp}\left( - \frac{1}{2 \sigma^2} \sum_i^n(x_i - \theta)^2\right )
        \end{equation}

        \begin{equation}
              \sum_i^n(x_i - \theta)^2 = \sum_i^n x_i^2 - 2 \theta \sum_i^n x_i + \theta^2; \hspace{10pt} \bar{x} = \frac{1}{n}\sum_i^n x_i
        \end{equation}

        \begin{equation}
              p(\theta|x_1,...,x_n) \propto \text{exp} \left( - \frac{n}{2 \sigma^2} (-2 \theta \bar{x} + \theta^2) - \frac{1}{2v} (\theta^2 - 2\theta m) \right)
        \end{equation}

        \begin{align}
              -\frac{1}{2} \left[ \left( \frac{n}{\sigma^2} + \frac{1}{v}\right) \theta^2 - 2 \left( \frac{n}{\sigma^2} \bar{x} + \frac{1}{v} m \right) \theta  \right] \\
              a=\frac{n}{\sigma^2} + \frac{1}{v}; \hspace{20pt} b = \frac{n}{\sigma^2} \bar{x} + \frac{1}{v}m \\
              -\frac{a}{2} \left [ \theta^2 - 2 \frac{b}{a} \theta \right] = -\frac{a}{2} \left[ \theta^2 - 2 \frac{b}{a} \theta + \frac{b^2}{a^2} - \frac{b^2}{a^2} \right ]
        \end{align}

        \begin{equation}
              p(\theta|x_1,...,x_n) \propto \text{exp} \left[ - \frac{a}{2} \left ( \theta - \frac{b}{a}\right ) ^2 \right]
        \end{equation}

        \begin{equation}
              \theta | x_1,...,x_n \sim \text{N} \left( \frac{\frac{n}{\sigma^2} \bar{x} + \frac{1}{v}m}{\frac{n}{\sigma^2} + \frac{1}{v}}, \left [ \frac{n}{\sigma^2} + \frac{1}{v}\right]^{-1} \right)
        \end{equation}

    \subsection*{D: Normal prior and sampling distribution with known mean and unknown variance}

        \begin{equation}
              p(\omega|x_1,..., x_n) \propto p(\omega|a,b)p(x_1,...,x_n|\omega)
        \end{equation}

        \begin{equation}
              p(\omega|a,b) = \frac{b^a}{\Gamma(a)} \omega^{a-1} \text{exp}(-b \omega)
        \end{equation}

        \begin{equation}
              p(x_i|\omega) = \left( \frac{\omega}{2 \pi}\right)^{1/2} \text{exp} \left [ -\frac{\omega}{2}(x_i - \theta)^2\right]
        \end{equation}

        \begin{equation}
              p(x_1,...,x_n|\omega) = \prod_i^n p(x_i|\omega) = \left( \frac{\omega}{2 \pi}\right)^{n/2} \text{exp} \left [ -\frac{\omega}{2}\sum_i^n(x_i - \theta)^2\right]
        \end{equation}

        \begin{equation}
              p(\omega|x_1,..., x_n) \propto \omega^{a-1} \text{exp}(-b \omega) \omega^{n/2} \text{exp} \left [ -\frac{\omega}{2}\sum_i^n(x_i - \theta)^2\right]
        \end{equation}

        \begin{equation}
              p(\omega|x_1,..., x_n) \propto \omega^{a + \frac{n}{2} - 1} \text{exp} \left [ -\omega \left (b+ \frac{1}{2}\sum_i^n(x_i - \theta)^2 \right )\right]
        \end{equation}

        \begin{equation}
              \omega | x_1,...,x_n \sim \text{Ga}\left (a + \frac{n}{2}, b+ \frac{1}{2}\sum_i^n(x_i - \theta)^2\right )
        \end{equation}

        \begin{equation}
              \sigma^2 | x_1,...,x_n \sim \text{IG}\left (a + \frac{n}{2}, b+ \frac{1}{2}\sum_i^n(x_i - \theta)^2\right )
        \end{equation}

    \subsection*{E: Normal prior and sampling distribution with unknown mean and known idiosyncratic variance}

        \begin{equation}
              p(\theta|x_1,..., x_n) \propto p(\theta|m,v)p(x_1,...,x_n|\theta)
        \end{equation}

        \begin{equation}
              p(\theta|m,v) = \left(\frac{1}{2 \pi v} \right )^{1/2} \text{exp}\left( - \frac{1}{2v} (\theta - m)^2\right )
        \end{equation}

        \begin{equation}
              p(x_i|\theta) = \left(\frac{\omega_i}{2 \pi} \right )^{1/2} \text{exp}\left( - \frac{\omega_i(x_i - \theta)^2}{2} \right )
        \end{equation}

        \begin{equation}
              p(x_1,...,x_n|\theta) = \prod_i^n p(x_i|\theta) \propto \text{exp}\left( - \frac{1}{2} \sum_i^n\omega_i(x_i - \theta)^2 \right )
        \end{equation}

        \begin{align}
            p(\theta|x_1,...,x_n) &\propto \text{exp}\left( - \frac{1}{2v} (\theta - m)^2\right) \text{exp}\left( - \frac{1}{2} \sum_i^n\omega_i(x_i - \theta)^2\right)\\
            &\propto \text{exp}\left(-\frac{1}{2v}(\theta - m)^2 - \frac{1}{2} \sum_i^n\omega_i(x_i - \theta)^2\right)
        \end{align}

        \begin{equation}
            -\frac{1}{2} \left[ \left(1 + \frac{1}{v} \right) \theta^2 - 2 \left ( \sum_i x_i \omega_i + \frac{1}{v}m\right) \theta\right]
        \end{equation}

        \begin{equation}
            \theta|x_1,...,x_n \sim \text{N}\left( \frac{\sum_i x_i w_i + \frac{1}{v}m}{1 + \frac{1}{v}}, \left [ 1 + \frac{1}{v}\right]^{-1}\right)
        \end{equation}


    \subsection*{F: Marginalization of likelihood with Gamma Prior and Normal Likelihood}

        \begin{equation}
              p(x) = \int_0^{\infty} p(x,\omega)d\omega = \int_0^{\infty} p(\omega)p(x|\omega)d\omega
        \end{equation}

        \begin{equation}
              p(\omega) = \frac{(b/2)^{a/2}}{\Gamma(a/2)}{\omega}^{a/2-1} \text{exp}\left (-\frac{b}{2}\omega \right)
        \end{equation}

        \begin{equation}
              p(x|\omega) = \left( \frac{\omega}{2 \pi}\right)^{1/2} \text{exp} \left [ -\frac{\omega}{2}(x - m)^2\right]
        \end{equation}

        \begin{equation}
              p(x,\omega) = \frac{(b/2)^{a/2}}{\Gamma(a/2)}  \left( \frac{1}{2 \pi}\right)^{1/2} (\omega)^{a/2+1/2-1}\text{exp}\left[-\omega \left( \frac{b}{2} + \frac{1}{2}(x - m)^2\right) \right]
        \end{equation}

        \begin{equation}
              a' = \frac{a+1}{2}; \hspace{20pt} b'= \frac{b}{2} + \frac{1}{2}(x-m)^2
        \end{equation}

        \begin{equation}
              p(x) = \int_0^{\infty}p(x, \omega) d\omega = \frac{(b/2)^{a/2}}{\Gamma(a/2) \sqrt{2\pi}} \frac{\Gamma(a')}{b'^{a'}} \int_0^{\infty} \frac{b'^{a'}}{\Gamma(a')}\omega^{a'-1} \text{exp}(-b' \omega) d\omega
        \end{equation}

        \begin{equation}
              p(x) = \frac{(b/2)^{a/2}}{\Gamma(a/2) \sqrt{2\pi}} \frac{\Gamma(\frac{a+1}{2})}{(b/2 + (x-m)^2/2)^{(a+1)/2}} \cdot \frac{(b/2)^{1/2}}{(b/2)^{1/2}}
        \end{equation}

        \begin{equation}
              \left( \frac{b}{2} + \frac{1}{2} (x-m)^2\right)^{-\frac{a+1}{2}} = \left (\frac{b}{2} \right)^{-\frac{a+1}{2}} \left( 1 + \frac{(x-m)^2}{b} \right)^{-\frac{a+1}{2}}
        \end{equation}

        \begin{equation}
              p(x) = \frac{\Gamma \left( \frac{a+1}{2}\right)}{\Gamma(\frac{a}{2})\sqrt{\pi b}} \left (1 + \frac{(x-m)^2}{b}\right)^{-\frac{a+1}{2}}
        \end{equation}


    \section*{The Multivariate Normal Distribution}

    \subsection*{A: Covariance of the multivariate normal}

        \begin{align}
            \text{cov}(x) &= \text{E}[(x-\mu)(x-\mu)^T ] = \text{E}[(x-\mu)_i (x-\mu)_j] = \text{E}[ x_i x_j - \mu_i x_j - x_i \mu_j + \mu_i \mu_j] \\
                &= \text{E}[x_i x_j] - \mu_i \text{E}[x_j] - \text{E}[x_i] \mu_j + \mu_i \mu_j \\
                &= \text{E}[x_i x_j] - \mu_i \mu_j
        \end{align}

        \begin{align}
            \text{cov}(Ax+b) &= \text{cov}(A_{ij}x_j + b_i) = \text{E}[(A_{ij}x_j + b_i)(A_{ji}x_i + b_j)]; \hspace{10pt} y_i = A_{ij}x_j; \hspace{10pt} y_j = A_{ji} x_i \\
            &= \text{E}[(y_i + b_i)(y_j + b_j)] = \text{E}[y_i y_j + y_i b_j + b_i y_j + b_i b_j] \\
            &\text{E}[y_i b_j] = \text{E}[y_i] \text{E}[b_j] \\
            &\text{E}[const] = 0 \\
        \end{align}

        \begin{equation}
            \text{cov}(A_{ij}x_j + b_i) = \text{E}[y_i y_j] = A_{ij} \text{E}[x_j x_i] A_{ji}
        \end{equation}

    \subsection*{B: PDF and moment-generating function of z}

        \begin{equation}
            z = (z_1,...,z_p)^T; \hspace{20pt} z_i \sim \text{N}(0,1)
        \end{equation}

        \begin{equation}
            M_{z_i}(t_i) = \text{E}[\text{exp}(t_i z_i)] = \int_{-\infty}^\infty \text{exp}(t_i z_i) \frac{1}{\sqrt{2 \pi}} \text{exp}\left( -\frac{1}{2} z_i^2 \right) dz_i
        \end{equation}

        \begin{equation}
            \text{exp}(t_i z_i) \text{exp}\left( -\frac{1}{2} z_i^2 \right) = \text{exp}\left( -\frac{1}{2} z_i^2 + z_i t_i\right) = \text{exp}\left( -\frac{1}{2}(z_i - t_i)^2 \right) \text{exp} \left (\frac{1}{2} t_i^2 \right)
        \end{equation}

        \begin{equation}
            M_{z_i}(t_i) = \text{exp}\left( \frac{1}{2} t_i^2\right) \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} \text{exp}\left( - \frac{1}{2} (z_i - t_i)^2\right) dz_i = \text{exp}\left( \frac{1}{2}t_i^2 \right )
        \end{equation}

        \begin{align}
            M_z(\mathbf{t}) &= \text{E}\left[\text{exp}\left(\sum_i z_i t_i\right)\right] = \text{E}\left[ \prod_i \text{exp}(z_i t_i)\right]\\
            &= \prod_i \text{E}[\text{exp}(z_i t_i)] = \prod_i M_{z_i}(t_i)\\
            &= \text{exp}\left( \sum_i \frac{1}{2} t_i^2 \right) = \text{exp}\left ( \frac{1}{2} \mathbf{t't}\right)
        \end{align}



\end{document}